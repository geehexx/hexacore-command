# pytest-codspeed Benchmarking Guide

## Metadata

| Key | Value |
| --- | --- |
| Topic | CodSpeed Benchmark Execution |
| Keywords | pytest, codspeed, benchmarking, performance |
| Related ADRs | [ADR-0005](../decisions/0005-performance-and-benchmarking-strategy.md) |
| Key Libraries | [`pytest-codspeed`](https://docs.codspeed.io/docs/pytest/overview) |

## Overview

CodSpeed integrates with pytest to provide statistically meaningful benchmark runs. This guide explains how Hexa-Core Command structures benchmarking code and invokes CodSpeed locally and in CI.

## Core Concepts

- **Benchmark Registry:** `BenchmarkRegistry` in `src/hexa_core/engine/benchmarking.py` manages benchmark callables. It exposes `run_with_pytest_codspeed()` so individual benchmarks can reuse CodSpeed's runner in tests.
- **Benchmark Suite Layout:** Benchmarks live under `tests/benchmarks/`. Register scenarios in Python modules and delegate execution to the registry. The new `tests/benchmarks/test_world_process_codspeed.py` module demonstrates end-to-end ECS world processing benchmarks.
- **Task Automation:** The `Taskfile.yml` targets `test:benchmarks` and `test:benchmarks:serial` run `pytest --codspeed` with and without `-n auto`, letting you toggle between parallel execution and deterministic debugging runs.

## Implementation Details

### Registering Benchmarks

```python
from hexa_core.engine.benchmarking import BenchmarkRegistry

registry = BenchmarkRegistry()

@registry.register("world_process")
def world_process():
    # perform the work to measure
    return engine.world.process()
```

### Executing via CodSpeed

```python
def test_benchmark_registry_with_codspeed(benchmark):
    results = registry.run_with_pytest_codspeed(benchmark)
    assert "world_process" in results
```

### Command-Line Usage

```bash
uv run pytest --codspeed -n auto tests/benchmarks/
uv run pytest --codspeed tests/benchmarks/
```

Or rely on Taskfile automation:

```bash
task test:benchmarks
task test:benchmarks:serial
```

### CI Integration

- `task ci:benchmarks` depends on `test:benchmarks`, ensuring CodSpeed benchmarks run in continuous integration.
- CodSpeed's statistical reports are automatically generated by the plugin; export artifacts as needed in GitHub Actions.

## Code Examples

- Reference implementation: `tests/spec/test_benchmark_registry_spec.py` validates `run_with_pytest_codspeed()` dispatch, including decorator-style registration safeguards.
- Use the registry to compose larger benchmark scenarios (ECS world processing, pathfinding) and call them from CodSpeed-enabled tests. See `tests/benchmarks/test_world_process_codspeed.py` for a concrete example.
